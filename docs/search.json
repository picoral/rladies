[
  {
    "objectID": "2023-09-15.html",
    "href": "2023-09-15.html",
    "title": "Reproducing Open Science Research 2",
    "section": "",
    "text": "September 15, 10am to 11am. Register to receive zoom link for meeting\nFor this tutorial, we will be replicating some of the analysis in Palma, P., Marin, M. F., Onishi, K. H., & Titone, D. (2022). Learning, inside and out: Prior linguistic knowledge and learning environment impact word learning in bilingual individuals. Language Learning, 72(4), 980-1016. Paper can be found at https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12501\nData, code, and more information can be found at https://osf.io/69seu/\n\nAbstract\n\nAlthough several studies have focused on novel word learning and lexicalization in (presumably) monolingual speakers, less is known about how bilinguals add novel words to their mental lexicon. In this study we trained 33 English–French bilinguals on novel word-forms that were neighbors to English words with no existing neighbors. The number of novel neighbors to each English word varied, as did the cross-linguistic orthographic overlap between the English word and its French translation. We assessed episodic memory and lexicalization of the novel words before and after a consolidation period. Cross-linguistic similarity enhanced episodic memory of novel neighbors only when neighborhood density among the novel neighbors was low. We also found evidence that novel neighbors of English words with high cross-linguistic similarity became lexicalized after a consolidation period. Overall, the results suggest that similarity to preexisting lexical representations crucially impacted lexicalization of novel words by bilingual individuals.\n\n\n\nData\nOur first step it to load the tidyverse library (RStudio will prompt you to install it).\n\n# load library for data analysis\nlibrary(tidyverse)\n\nNow we can read the data in. Remember to download the file and place it in a directory called data in your project. We will filter any missing data as we read the data in. The dependent variable is ACC (binary, for accuracy).\n\ndata &lt;- read_csv(\"data/OSF_word_learning_FC.csv\") %&gt;% \n  filter(!is.na(ACC))\n\nRows: 2560 Columns: 35\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (13): Experiment, Training_group, Trainedset, L1, L2, L3, L4, Stimuli, c...\ndbl (21): Subject, nb_languages_known, L1_AoA, L1_subjective_fluency, L2_sub...\nnum  (1): L2_AoA\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nParticipants\n\nOne participant was excluded because of an error in their assignment to the testing condition. Thus, 32 participants were included in the analyses reported below.\n\nThere are multiple measures per participant, so we need to run distinct() on the Subject variable to get number of participants. The nrow() tells use how many row (in this case each row/observation is a participant).\n\ndata %&gt;% \n  distinct(Subject) %&gt;% \n  nrow()\n\n[1] 32\n\n\nWe can look at how participants are distributed across experiments and training groups, also using distinct()\n\ndata %&gt;% \n  distinct(Experiment, Subject, Training_group) %&gt;% \n  count(Experiment)\n\n\n\n\n\nExperiment\nn\n\n\n\n\nday1-2\n15\n\n\nday1-8\n17\n\n\n\n\n\n\n\ndata %&gt;% \n  distinct(Experiment, Subject, Training_group) %&gt;% \n  count(Training_group)\n\n\n\n\n\nTraining_group\nn\n\n\n\n\ns1\n12\n\n\ns2\n10\n\n\ns3\n10\n\n\n\n\n\n\n\nall participants indicated maximal proficiency in English (9)\n\nUsing distinct() again, we will look at their reported L1 fluency.\n\ndata %&gt;%\n  distinct(Subject, L1_subjective_fluency) %&gt;% \n  summarize(mean(L1_subjective_fluency))\n\n\n\n\n\nmean(L1_subjective_fluency)\n\n\n\n\n9\n\n\n\n\n\n\n\nThere was more variability in terms of French proficiency, mean = 4.69, SD = 2.09, 95% CI [3.97, 5.41], range = 1–9.\n\nLet’s replicate these stats (don’t forget distinct()):\n\ndata %&gt;%\n  distinct(Subject, L2_subjective_fluency) %&gt;% \n  summarize(n = n(),\n            mean = mean(L2_subjective_fluency),\n            sd = sd(L2_subjective_fluency),\n            min = min(L2_subjective_fluency),\n            max = max(L2_subjective_fluency),\n            se = qt(0.975, n - 1) * mean/sqrt(n),\n            upper = mean + se,\n            lower = mean - se)\n\n\n\n\n\nn\nmean\nsd\nmin\nmax\nse\nupper\nlower\n\n\n\n\n32\n4.96875\n2.071066\n1\n9\n1.791425\n6.760175\n3.177325\n\n\n\n\n\n\n\nThe mean age of acquisition of French was 8.58 years, SD = 4.36, 95% CI [7.07, 10.10], range = 3–18, indicating that all of the participants were sequential bilinguals.\n\n\ndata %&gt;% \n  distinct(Subject, L2_AoA) %&gt;% \n  summarize(n = n(),\n            mean = mean(L2_AoA),\n            sd = sd(L2_AoA),\n            min = min(L2_AoA),\n            max = max(L2_AoA),\n            se = qt(0.975, n - 1) * mean/sqrt(n),\n            upper = mean + se,\n            lower = mean - se)\n\n\n\n\n\nn\nmean\nsd\nmin\nmax\nse\nupper\nlower\n\n\n\n\n32\n8.3125\n6.098321\n1\n35\n2.996976\n11.30948\n5.315524\n\n\n\n\n\n\n\n\nGeneralized linear mixed-effects model\n\nFixed effects included session for forced-choice timing (FC A, before a consolida- tion period, vs. FC B, after a consolidation period), number of novel neighbors associated with the base word during training (1 vs. 5), and NLD (measure of orthographic distance for translation equivalents across languages). Their in- teractions were also included. Categorical predictors were effects-coded (−.5 vs. .5) to allow for the interpretation of main effects. NLD was standardized with mean of 0 and standard deviation of 1. […] We also added English frequency as a stan- dardized covariate, as frequency is a strong predictor of the speed of lexical access during language processing (e.g., Whitford & Titone, 2012). […] The final random effects structure included a random intercept by participants, a random intercept by items, a correlated random slope for forced-choice task timing (before vs. af- ter consolidation) by participants, and a correlated random slope for number of neighbors (one neighbor vs. five neighbors) by participants.\n\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(lmerTest)\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\ndata &lt;- data %&gt;% \n  mutate(NLD_scaled = scale(NLD),\n         Frequency_scaled = scale(Frequency))\n\nmodel &lt;- glmer(ACC ~ nb_neighbors_dev * FC_dev * NLD_scaled + Exp_dev + Frequency_scaled +\n               (1 + nb_neighbors_dev + FC_dev|Subject) + (1|Stimuli), \n               family = binomial, \n               data = data)\n\nsummary(model)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nACC ~ nb_neighbors_dev * FC_dev * NLD_scaled + Exp_dev + Frequency_scaled +  \n    (1 + nb_neighbors_dev + FC_dev | Subject) + (1 | Stimuli)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2673.7   2772.4  -1319.8   2639.7     2443 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0821 -0.7103  0.4327  0.6368  1.4226 \n\nRandom effects:\n Groups  Name             Variance Std.Dev. Corr       \n Stimuli (Intercept)      0.006412 0.08007             \n Subject (Intercept)      0.276048 0.52540             \n         nb_neighbors_dev 0.200890 0.44821   0.05      \n         FC_dev           0.262732 0.51257  -0.19  0.90\nNumber of obs: 2460, groups:  Stimuli, 60; Subject, 32\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         1.21532    0.10795  11.258  &lt; 2e-16 ***\nnb_neighbors_dev                   -0.38385    0.13009  -2.951  0.00317 ** \nFC_dev                             -0.54121    0.13811  -3.919 8.91e-05 ***\nNLD_scaled                          0.02888    0.05127   0.563  0.57329    \nExp_dev                            -0.55439    0.21206  -2.614  0.00894 ** \nFrequency_scaled                    0.04498    0.05219   0.862  0.38879    \nnb_neighbors_dev:FC_dev             0.54905    0.20483   2.681  0.00735 ** \nnb_neighbors_dev:NLD_scaled        -0.23163    0.09757  -2.374  0.01760 *  \nFC_dev:NLD_scaled                  -0.03842    0.09730  -0.395  0.69291    \nnb_neighbors_dev:FC_dev:NLD_scaled  0.07793    0.19472   0.400  0.68900    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) nb_ng_ FC_dev NLD_sc Exp_dv Frqnc_ nb__:FC_ n__:NL FC_:NL\nnb_nghbrs_d -0.018                                                          \nFC_dev      -0.168  0.419                                                   \nNLD_scaled   0.013 -0.046 -0.011                                            \nExp_dev     -0.091  0.017  0.038 -0.007                                     \nFrqncy_scld  0.010 -0.003  0.004  0.239 -0.012                              \nnb_ngh_:FC_  0.045 -0.113 -0.077  0.030 -0.029  0.010                       \nnb_ng_:NLD_ -0.031  0.017  0.023 -0.046  0.007  0.023 -0.021                \nFC_dv:NLD_s -0.007  0.022  0.015 -0.104  0.004 -0.017 -0.059    0.053       \nn__:FC_:NLD  0.014 -0.013 -0.045  0.053 -0.002  0.006  0.021   -0.106 -0.054\n\n\nThe authors use this report library. I’m not sure I’d recommend it myself.\n\nlibrary(report)\nreport(model)\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer)\nto predict ACC with nb_neighbors_dev, FC_dev, NLD_scaled, Exp_dev and\nFrequency_scaled (formula: ACC ~ nb_neighbors_dev * FC_dev * NLD_scaled +\nExp_dev + Frequency_scaled). The model included nb_neighbors_dev as random\neffects (formula: list(~1 + nb_neighbors_dev + FC_dev | Subject, ~1 |\nStimuli)). The model's total explanatory power is moderate (conditional R2 =\n0.16) and the part related to the fixed effects alone (marginal R2) is of 0.06.\nThe model's intercept, corresponding to nb_neighbors_dev = 0, FC_dev = 0,\nNLD_scaled = 0, Exp_dev = 0 and Frequency_scaled = 0, is at 1.22 (95% CI [1.00,\n1.43], p &lt; .001). Within this model:\n\n  - The effect of nb neighbors dev is statistically significant and negative\n(beta = -0.38, 95% CI [-0.64, -0.13], p = 0.003; Std. beta = -0.19, 95% CI\n[-0.32, -0.07])\n  - The effect of FC dev is statistically significant and negative (beta = -0.54,\n95% CI [-0.81, -0.27], p &lt; .001; Std. beta = -0.27, 95% CI [-0.41, -0.14])\n  - The effect of NLD scaled is statistically non-significant and positive (beta\n= 0.03, 95% CI [-0.07, 0.13], p = 0.573; Std. beta = 0.03, 95% CI [-0.07,\n0.13])\n  - The effect of Exp dev is statistically significant and negative (beta =\n-0.55, 95% CI [-0.97, -0.14], p = 0.009; Std. beta = -0.28, 95% CI [-0.48,\n-0.07])\n  - The effect of Frequency scaled is statistically non-significant and positive\n(beta = 0.04, 95% CI [-0.06, 0.15], p = 0.389; Std. beta = 0.04, 95% CI [-0.06,\n0.15])\n  - The effect of nb neighbors dev × FC dev is statistically significant and\npositive (beta = 0.55, 95% CI [0.15, 0.95], p = 0.007; Std. beta = 0.14, 95% CI\n[0.04, 0.24])\n  - The effect of nb neighbors dev × NLD scaled is statistically significant and\nnegative (beta = -0.23, 95% CI [-0.42, -0.04], p = 0.018; Std. beta = -0.12,\n95% CI [-0.21, -0.02])\n  - The effect of FC dev × NLD scaled is statistically non-significant and\nnegative (beta = -0.04, 95% CI [-0.23, 0.15], p = 0.693; Std. beta = -0.02, 95%\nCI [-0.11, 0.08])\n  - The effect of (nb neighbors dev × FC dev) × NLD scaled is statistically\nnon-significant and positive (beta = 0.08, 95% CI [-0.30, 0.46], p = 0.689;\nStd. beta = 0.02, 95% CI [-0.08, 0.11])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\nreport_effectsize(model)\n\nEffect sizes were labelled following Chen's (2010) recommendations. \n\nsmall (Std. beta = 1.21, 95% CI [1.00, 1.42])\nvery small (Std. beta = -0.19, 95% CI [-0.32, -0.07])\nvery small (Std. beta = -0.27, 95% CI [-0.41, -0.14])\nvery small (Std. beta = 0.03, 95% CI [-0.07, 0.13])\nvery small (Std. beta = -0.28, 95% CI [-0.48, -0.07])\nvery small (Std. beta = 0.04, 95% CI [-0.06, 0.15])\nvery small (Std. beta = 0.14, 95% CI [0.04, 0.24])\nvery small (Std. beta = -0.12, 95% CI [-0.21, -0.02])\nvery small (Std. beta = -0.02, 95% CI [-0.11, 0.08])\nvery small (Std. beta = 0.02, 95% CI [-0.08, 0.11])\n\n\n\\(R^2\\) (read r squared) shows how much of the variance in the data is explained by the idependent variables in the model.\n\nlibrary(MuMIn)\nr.squaredGLMM(model)\n\nWarning: 'r.squaredGLMM' now calculates a revised statistic. See the help page.\n\n\nWarning: the null model is correct only if all variables used by the original\nmodel remain unchanged.\n\n\n                   R2m       R2c\ntheoretical 0.05668291 0.1592204\ndelta       0.03687604 0.1035836"
  }
]